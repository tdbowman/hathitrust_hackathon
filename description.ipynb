{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Mind Map of Hackathon Project Mashup](images/Mind%20map%20with%20lines.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# DATA REQUIREMENTS #\n",
        "\n",
        "1. Utilize Hathi Trust data APIs including new FEATURES API\n",
        "\n",
        "2. Mash up Open Syllabi data\n",
        "\n",
        "3. Mash up Open Library data\n",
        "\n",
        "4. Mash up Goodreads data\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "\n",
        "# OVERALL PROCESS #\n",
        "\n",
        "1. First, we ensure we have all Python libraries necessary\n",
        "\n",
        "2. We utilize a list from a GitHub project using data from Open Syllabus (https://raw.githubusercontent.com/mtdamir/open-syllabus/master/urls.csv) and grab the \"Classical\" entries.\n",
        "\n",
        "3. We grab the JSON file from Open Syllabus for each entry\n",
        "\n",
        "4. We search the Open Syllabus JSON record for either \"id_hathi_trust\" or \"isbns\"\n",
        "    1. If there is no HATHI TRUST ID, we grab only the *first* ISBN as proof of concept\n",
        "\n",
        "5. We query the HATHI TRUST metadata API using either the HATHI TRUST ID or ISBNS from step \n",
        "\n",
        "6. We query the GOODREADS API (deprecated but still working) to get ranking of book if available using GOODREADS link from OPEN SYLLABUS json \n",
        "\n",
        "7. We count tokens, sentences, lines, and syllables per volume to calculate Fleshman Readability and Grade Level scores using HATHI TRUST FEATURES json\n",
        "\n",
        "8. We visualize the academic domains where the volume has appeared using OPEN SYLLABUS json record\n",
        "\n",
        "\n",
        "------\n",
        "\n",
        "# OVERALL GOALS #\n",
        "\n",
        "1. Determine how Hathi Trust books are used in Syllabi for educational purposes\n",
        "\n",
        "2. Determine what academic domains are using the documents\n",
        "\n",
        "3. Deteremine how difficult the documents are to read\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANF2pBlvAhIP",
        "outputId": "853cfaf3-897b-4ade-c944-b7194299988c"
      },
      "outputs": [],
      "source": [
        "!pip install htrc-feature-reader\n",
        "!pip install syllaby\n",
        "# !pip install htrc\n",
        "# !pip install isbnlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP ONE ##\n",
        "\n",
        "1. import libraries needed\n",
        "\n",
        "2. write functions for assisting with processing\n",
        "\n",
        "3. load TXT file (list of CLASSICS documents from Open Syllabus)\n",
        "    1. count number of lines in file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iL1OZNB9v4E",
        "outputId": "2c86a104-f4e5-4f0d-e4cb-caaee03da3f1"
      },
      "outputs": [],
      "source": [
        "from lxml import etree\n",
        "import os\n",
        "from urllib.request import urlopen\n",
        "import json\n",
        "from htrc_features import *\n",
        "from htrc import workset, metadata\n",
        "from time import sleep\n",
        "from tqdm import tqdm\n",
        "from isbnlib import mask, to_isbn10, is_isbn10, is_isbn13\n",
        "import re\n",
        "import linecache\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "import syllapy\n",
        "from plotnine import *\n",
        "\n",
        "def get_link(link):\n",
        "    \"\"\" \n",
        "    Retrieve online document\n",
        "\n",
        "    Args:\n",
        "        link (STR): valid URL\n",
        "    \n",
        "    Return:\n",
        "        r (OBJ): request object\n",
        "    \"\"\"\n",
        "    \n",
        "    r = requests.get(link)\n",
        "    return(r)\n",
        "\n",
        "def get_htid(link):\n",
        "    \"\"\"\n",
        "    Grab htids, year, and title from JSON record\n",
        "\n",
        "    Args:\n",
        "        link (STR): link to HATHI TRUST API\n",
        "        \n",
        "    Return:\n",
        "        matched (BOOL): did we find document in HATHI TRUST DATASET\n",
        "        htids (LIST or empty list): all htids we find\n",
        "        year (LIST or 0): all publishDates we find\n",
        "        title (LIST or \"\"): all titles we find     \n",
        "    \n",
        "    \"\"\"\n",
        "    r = get_link(link)\n",
        "    data = r.json()\n",
        "    if len(data[\"items\"]) > 0:\n",
        "        matched = True\n",
        "        htids = [(value['htid']) for value in data[\"items\"]]\n",
        "        year = [(value['publishDates']) for value in data[\"records\"].values()][0]\n",
        "        title = [(value['titles']) for value in data[\"records\"].values()][0]\n",
        "    else:\n",
        "        matched = False\n",
        "        htids = []\n",
        "        year = 0\n",
        "        title = \"\"\n",
        "      \n",
        "    return(htids, year, title, matched)\n",
        "\n",
        "def get_subject(link):\n",
        "    \"\"\"\n",
        "    Grab MARC subject codes from HATHI TRUST metadata API and use loc.gov/MARC21 to grab apporpiate fields\n",
        "\n",
        "    Args:\n",
        "        link (STR): HATHI TRUST metadata API link\n",
        "        \n",
        "    Return:\n",
        "        subject_list (LIST): subjects from MARC record and retrieve appropriate info from loc.gov/MARC21\n",
        "        subject_no (LIST):  subject codes from MARC record \n",
        "    \"\"\"\n",
        "    r = get_link(link)\n",
        "    data = r.json()\n",
        "    result = [(value['marc-xml']) for value in data[\"records\"].values()][0]\n",
        "    result = result.replace('encoding=\"UTF-8\"', \"\")\n",
        "    root = etree.fromstring(result)\n",
        "    \n",
        "    subject_list = []\n",
        "    for record in root:\n",
        "      fields = ['600', '610', '611', '630', '650', '651', '655']\n",
        "      subject_no = 0\n",
        "      for field in fields:\n",
        "        query = \"\".join(['{http://www.loc.gov/MARC21/slim}datafield[@tag=\"', field, '\"]'])\n",
        "        subjects = record.findall(query)\n",
        "        for item in subjects:\n",
        "          subfields = item.findall('{http://www.loc.gov/MARC21/slim}subfield')\n",
        "          Ind2 = item.get(\"ind2\")\n",
        "          if Ind2 == \"0\":\n",
        "            for s in subfields:\n",
        "              if s.get('code') == 'a':\n",
        "                  value = s.text\n",
        "                  subject_list.append(value)\n",
        "                  \n",
        "    return([subject_list, subject_no])\n",
        "\n",
        "\n",
        "# Calulate Flesch–Kincaid readability tests\n",
        "def calculate_readability(num_words, num_sentences, num_syllables):\n",
        "    \"\"\"\n",
        "    Calulate Flesch–Kincaid readability tests and grade level\n",
        "\n",
        "    Args:\n",
        "        num_words (INT): number of tokens for volume\n",
        "        num_sentences (INT): number of sentences for volume\n",
        "        num_syllables (INT): number of syllables for volume\n",
        "\n",
        "    Returns:\n",
        "        readability_score (FLOAT): Flesch–Kincaid readability score\n",
        "        grade_level (FLOAT): Flesch–Kincaid grade level score\n",
        "    \"\"\"\n",
        "\n",
        "    readability_score = 206.835 - 1.015 * (num_words/num_sentences) - 84.6 * (num_syllables/num_words)\n",
        "    grade_level = 0.39 * (num_words / num_sentences) + 11.8 * (num_syllables/num_words) - 15.59\n",
        "    \n",
        "    return readability_score, grade_level\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### GET OUR STARTING FILE ###\n",
        "\n",
        "# Open file and read line by line\n",
        "file1 = open('/content/classics_list.txt', 'r')\n",
        "\n",
        "# count lines in file\n",
        "count = sum( 1 for line in file1 )\n",
        "#print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yeh4gr0ODPg",
        "outputId": "2ade45e7-41dd-459c-9061-88b38b195795"
      },
      "outputs": [],
      "source": [
        "# set up blank variables\n",
        "data_summary = []\n",
        "htid_lists = []\n",
        "subject_lists = []\n",
        "\n",
        "for i in tqdm(range(1, count)):\n",
        "    \n",
        "    # Get next line\n",
        "    url = linecache.getline('/content/classics_list.txt', i)\n",
        "    json_file_name = url.split(\"/\")[5].split(\".\")[0]\n",
        "\n",
        "    # if line is empty\n",
        "    # end of file is reached\n",
        "    if not url:\n",
        "        break\n",
        "\n",
        "    # open url and store response\n",
        "    response = urlopen(url)\n",
        "\n",
        "    # storing the JSON response\n",
        "    # from url in data\n",
        "    data_json = json.loads(response.read())\n",
        "    \n",
        "    # iterating through JSON \n",
        "    if data_json['availability']['Open Library']:\n",
        "        \n",
        "\n",
        "        # grab open library URL\n",
        "        #https://openlibrary.org/search.json\n",
        "\n",
        "        open_lib_url = data_json['availability']['Open Library']\n",
        "        #print(open_lib_url)\n",
        "        \n",
        "        # change URL to give us JSON record\n",
        "        new_open_lib_url = open_lib_url.replace('search?', 'search.json?')\n",
        "\n",
        "        # request Open Library data\n",
        "        r = requests.get(new_open_lib_url)\n",
        "        ol_data = r.json()\n",
        "\n",
        "        # store file\n",
        "        ol_filename = \"\".join(['./open_library_json/', json_file_name, '.json'])\n",
        "        try:\n",
        "          with open(ol_filename, 'w+') as file:\n",
        "            json.dump(ol_data, file)\n",
        "        except FileExistsError:\n",
        "          print(f\"The file '{file_path}' already exists.\")\n",
        "\n",
        "\n",
        "        # store the response of URL\n",
        "        response = urlopen(new_open_lib_url)\n",
        "        data_json = json.loads(response.read())\n",
        "\n",
        "        # see what we have (HATHI TRUST ID -or- ISBNS)\n",
        "        try:\n",
        "          rnumbers = data_json['docs'][0]['id_hathi_trust']\n",
        "        except:\n",
        "          rnumbers = []\n",
        "        try:\n",
        "          isbns = data_json['docs'][0]['isbn']\n",
        "        except:\n",
        "          isbns = []\n",
        "\n",
        "        # see how many HATHI TRUST and ISBNS we have\n",
        "        rnumber_count = len(rnumbers)\n",
        "        isbn_count = len(isbns)\n",
        "\n",
        "        # HATHI TRUST ID\n",
        "        if rnumber_count > 0:\n",
        "          print(rnumbers[0])\n",
        "          link = \"\".join([\"https://catalog.hathitrust.org/api/volumes/full/recordnumber/\",\n",
        "                          rnumbers[0], \".json\"])\n",
        "        # ISBNS\n",
        "        elif isbn_count > 0:\n",
        "          #print(isbns[0])\n",
        "          link = \"\".join([\"https://catalog.hathitrust.org/api/volumes/full/isbn/\",\n",
        "                          isbns[0], \".json\"])\n",
        "        #print(link)\n",
        "        \n",
        "        # Grab htids, year, and title from JSON record using our function\n",
        "        htids = get_htid(link)\n",
        "\n",
        "        # grab JSON file\n",
        "        r = requests.get(link)\n",
        "        data = r.json()\n",
        "\n",
        "        # save JSON file\n",
        "        ht_filename = \"\".join(['./hathitrust_json/', json_file_name, '.json'])\n",
        "        try:\n",
        "          with open(ht_filename, 'w+') as file:\n",
        "            json.dump(data, file)\n",
        "        except FileExistsError:\n",
        "          print(f\"The file '{file_path}' already exists.\")\n",
        "\n",
        "\n",
        "        # just to summarize our data\n",
        "        data_summary.append({\n",
        "            \"osurl\":url,\n",
        "            \"ht_link\":link,\n",
        "            \"ht_matched\":htids[3],\n",
        "            \"htid_count\":len(htids[0]),\n",
        "            \"year\":htids[1],\n",
        "            'title':htids[2],\n",
        "            \"subject_no\":subjects[1]\n",
        "        })\n",
        "\n",
        "\n",
        "        # Get SUBJECTS\n",
        "        if htids[3] == True:\n",
        "\n",
        "          subjects = get_subject(link)\n",
        "\n",
        "          for item in htids[0]:\n",
        "              htid_lists.append({\n",
        "                  \"osurl\":url,\n",
        "                  \"ht_link\":link,\n",
        "                  \"htid\":item\n",
        "              })\n",
        "          for item in subjects[0]:\n",
        "            subject_lists.append({\n",
        "                \"osurl\":url,\n",
        "                \"subject\":item\n",
        "            })\n",
        "\n",
        "\n",
        "        # sleep because of Open Syllabus API\n",
        "        time.sleep(2)\n",
        "\n",
        "# close our file\n",
        "file1.close()\n",
        "\n",
        "\n",
        "### SUMMARIZE WHAT WE'VE ACCOMPLISHED ###\n",
        "# use Pandas dataframe\n",
        "data_summary_df = pd.DataFrame(data_summary)\n",
        "\n",
        "print(len(data_summary))\n",
        "print(sum(data_summary_df['ht_matched']))\n",
        "print(len(htid_lists))\n",
        "print(len(subject_lists))\n",
        "\n",
        "data_summary_df.head()\n",
        "\n",
        "htid_df = pd.DataFrame(htid_lists)\n",
        "htid_df.head()\n",
        "\n",
        "subject_df = pd.DataFrame(subject_lists)\n",
        "subject_df.head()\n",
        "\n",
        "data_summary_df.to_csv(\"data_summary.csv\")\n",
        "htid_df.to_csv(\"htid_list.csv\")\n",
        "subject_df.to_csv(\"subject_list.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP TWO ##\n",
        "\n",
        "1. Utilize JSON files saved in STEP ONE to analyze our dataset\n",
        "\n",
        "2. Determine total number of academic domains where each document has appeared and graph\n",
        "\n",
        "3. Determine readability score and grade level score for all documents\n",
        "\n",
        "4. Determine subject lists for each book and see how subjects map to academic domains\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# VISUALIZE BAR CHART OF ACADEMIC DOMAINS using OPEN SYLLABUS API data\n",
        "keys, vals = [], []\n",
        "data = data_json['syllabiByField']\n",
        "for i in data:\n",
        "    keys.append(i['name'])\n",
        "    vals.append(i['count'])\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(keys, vals, color ='maroon', \n",
        "        width = 0.4)\n",
        "\n",
        "plt.xlabel(\"Domain\")\n",
        "plt.ylabel(\"No. of syllabi\")\n",
        "plt.title(f\"Use of {book_title} across domains in syllabi\")\n",
        "\n",
        "# Add the values of each bar to the chart\n",
        "for i, v in enumerate(vals):\n",
        "    plt.text(i, v + 5, str(v))\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![\"Open Syllabus areas for documen\"](images/bar_chart_openSyllabus.png \"Open Syllabus areas for document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#VISUALIZE GOODREADS STAR RANKING#\n",
        "def draw_stars(n):\n",
        "    \"\"\"\n",
        "    Draw stars for ranking representation\n",
        "\n",
        "    Args:\n",
        "        n (FLOAT): Goodreads score ranking\n",
        "    \"\"\"\n",
        "    for i in range(round(n)):\n",
        "        plt.plot(i, 0, marker='*', markersize=20, color='magenta')\n",
        "\n",
        "\n",
        "# get goodreads ranking\n",
        "goodreads_ranking = float(data_goodreads_json['books'][0]['average_rating'])\n",
        "draw_stars(goodreads_ranking)\n",
        "\n",
        "# add title to visualization\n",
        "plt.title(f\"Goodreads Ranking {goodreads_ranking}\")\n",
        "ax = plt.gca()\n",
        "\n",
        "# remove frame box\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "# Hide X and Y axes label marks\n",
        "ax.xaxis.set_tick_params(labelbottom=False)\n",
        "ax.yaxis.set_tick_params(labelleft=False)\n",
        "\n",
        "# Hide X and Y axes tick marks\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "# Set the axes limits\n",
        "plt.xlim(-1, 15)\n",
        "plt.ylim(-1, 2)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](images/goodreads_rating.png \"Goodreads Rating for document\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count all tokens, sentences, lines for each document\n",
        "# Count syllables for each token and add them up\n",
        "# Calculate Flesch–Kincaid readability tests\n",
        "\n",
        "syllapy_count, token_count, sentence_count, syllable_count, line_count = 0,0,0,0,0\n",
        "for page in data_hathi_ef_json['data']['features']['pages']:\n",
        "\n",
        "    body = page.get('body')\n",
        "    if type(body) is dict:\n",
        "        if body['tokenCount']:\n",
        "            token_count += int(body['tokenCount'])\n",
        "        if body['sentenceCount']:\n",
        "            sentence_count += int(body['sentenceCount'])\n",
        "        if body['lineCount']: \n",
        "            line_count += int(body['lineCount'])\n",
        "\n",
        "        tokens = list(body['tokenPosCount'].keys())\n",
        "\n",
        "        \n",
        "        for token in tokens:\n",
        "                syllapy_count += syllapy.count(token)\n",
        "\n",
        "\n",
        "#print(\"\\n\"*4)\n",
        "#print(token_count)\n",
        "#print(sentence_count)\n",
        "#print(line_count)\n",
        "#print(syllapy_count)\n",
        "\n",
        "readability, grade_level = calculate_readability(token_count, sentence_count, syllapy_count)        \n",
        "print(f\"READABILITY SCORE: {readability}\")\n",
        "print(f\"GRADE LEVEL: {grade_level}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](images/ReadingScores.png \"Document reading score and grade level\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Subjects from Hathi Trust MARC21 record #\n",
        "subject = pd.read_csv(\"subject_list.csv\")\n",
        "subject = subject.drop_duplicates()\n",
        "\n",
        "subject['subject_new'] = [re.sub('\\.','', str(x)) for x in subject['subject']]\n",
        "subject['subject_new'].value_counts().reset_index().head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](images/subject_across_all_matched.png \"All LCSH for all matched documents\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot matched documents by year\n",
        "f1 = (\n",
        "ggplot(data, aes(x = \"year_new\", y = \"..count..\")) +\n",
        "    geom_bar(width = 1) +\n",
        "    xlim(1900, 2020) +\n",
        "    labs(x = 'Publication Year', y = \"Counts\",\n",
        "         title = \"Publication Years of Books Matched to HathiTrust\") +\n",
        "    theme_linedraw()\n",
        ")\n",
        "f1\n",
        "ggsave(f1, \"f1.jpg\", dpi = 400, width = 5, height = 3.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![alt text](images/f1.jpg \"All matched documents across years\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
